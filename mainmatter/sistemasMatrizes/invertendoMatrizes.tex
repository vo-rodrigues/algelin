\section{Invertendo matrizes}

O processo de escalonamento é uma ferramenta que é útil, também, para inverter matrizes.

Primeiro, definiremos invertibilidades laterais.

\begin{definition}
    Seja $R$ um anel e $a\in M_{n}(R)$.
    Dizemos que $a$ é \textbf{invertível à esquerda} se existe $b\in M_{n}(R)$ tal que $ba=I_n$.
    Nesse caso, $b$ é chamado de um \textbf{inverso à esquerda} de $a$.

    Dizemos que $a$ é \textbf{invertível à direita} se existe $c\in M_{n}(R)$ tal que $ac=I_n$.
    Nesse caso, $c$ é chamado de um \textbf{inverso à direita} de $a$.
\end{definition}

Quando inversos laterais de ambos os lados existem, eles coincidem, e, portanto, o elemento é invertível.

\begin{proposition}
    Seja $R$ um anel e $a\in M_{n}(R)$.
    Se $a$ é invertível à esquerda e à direita, então $a$ é invertível e os inversos laterais coincidem.
\end{proposition}
\begin{proof}
    Suponha que $b$ é um inverso à esquerda de $a$ e que $c$ é um inverso à direita de $a$.
    Então, temos
   \begin{equation*}
        b = b1 = b(ac) = (ba)c = 1c = c.
   \end{equation*}
    Portanto, $b=c$, e assim $a$ é invertível com (único) inverso $b=c$.
\end{proof}

Veremos mais adiante neste texto que há anéis para os quais existem elementos invertíveis apenas de um lado.
Porém, conforme veremos a seguir, isso não ocorre para matrizes quadradas sobre corpos.

\begin{lemma}
    Seja $A\in M_{n}(\mathbb K)$.
    São equivalentes:
    \begin{enumerate}[label=(\roman*)]
        \item $A$ é invertível.
        \item $A$ é invertível à esquerda.
        \item O sistema linear homogêneo $AX=0$ possui apenas a solução trivial.
        \item Existe uma sequência finita de operações elementares nas linhas de $M_{n}(\mathbb K)$ que transforma $A$ na matriz identidade $I_n$.
        \item $A$ pode ser escrita como um produto de matrizes elementares.
    \end{enumerate}
\end{lemma}

\begin{proof} Façamos a demonstração das implicações.
    \begin{itemize}
        \item[(i)$\Rightarrow$(ii)] Imediato.

        \item[(ii)$\Rightarrow$(iii)] Suponha que $A$ é invertível à esquerda.
            Então, existe $B\in M_{n}(\mathbb K)$ tal que $BA=I_n$.

            Seja $\alpha=(\alpha_1, \dots, \alpha_n)\in \mathbb K^n$ uma solução do sistema linear homogêneo $AX=0$.
            Segue que:
            \begin{equation*}
                A\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix} = 0.
            \end{equation*}
            Multiplicando ambos os lados da equação por $B$ à esquerda, obtemos \begin{equation*}
                BA\begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix} = B0=0.
            \end{equation*}
            Ou seja, $\alpha_1=\alpha_2=\cdots=\alpha_n=0$.
            Portanto, o sistema linear homogêneo $AX=0$ possui apenas a solução trivial.

        \item[(iii)$\Rightarrow$(iv)] Suponha que o sistema linear homogêneo $AX=0$ possui apenas a solução trivial.
        Existem operações elementares nas linhas que transformam $A$ em uma matriz escalonada $S$.

        Como o sistema linear homogêneo $AX=0$ possui apenas a solução trivial, todas as colunas de $S$ possuem pivôs.
        
        Como o número de linhas de $S$ é igual ao número de colunas de $S$, segue que toda linha $i$ de $S$ possui pivô.

        Sejam $j_1, j_2, \dots, j_n$ as posições dos pivôs de $S$ das linhas $1, 2, \dots, n$, respectivamente.
        Temos que $1\leq j_1 < j_2 < \cdots < j_n \leq n$.
        Portanto, $j_k=k$ para todo $k=1, 2, \dots, n$.
        Assim, $S$ é uma matriz cuja diagonal é composta de pivôs, e, os elementos fora da diagonal, por estarem em uma coluna com pivô, são nulos.
        Ou seja, $S$ é a matriz identidade.

        \item[(iv)$\Rightarrow$(v)] Por hipótese, existem operações elementares $e_1, \dots, e_k$ tais que $e_k\circ \dots e_1(A)=I_n$.
        
        Sendo $E_1, \dots, E_k$ as matrizes elementares correspondentes às operações elementares $e_1, \dots, e_k$, respectivamente, temos que:
        \begin{equation*}
            E_k \cdots E_1 A = I_n.
        \end{equation*}

        Como as matrizes elementares são invertíveis, segue que $A = E_1^{-1} \cdots E_k^{-1}$.

        Como o inverso de uma matriz elementar é uma matriz elementar, segue que $A$ pode ser escrita como um produto de matrizes elementares.

        \item[(v)$\Rightarrow$(i)] Suponha que $A$ pode ser escrita como um produto de matrizes elementares.
        Como as matrizes elementares são invertíveis, segue que $A$ é um produto de matrizes invertíveis, e, portanto, $A$ é invertível.
    \end{itemize}
\end{proof}

Notemos um fato interessante: na demonstração anterior, a implicação (iv)$\Rightarrow$(v) nos dá um algoritmo sobre como escrever uma matriz invertível como um produto de matrizes elementares.

Vejamos um exemplo de seu funcionamento.

\begin{example}
    Seja 
    \begin{equation*}
        A = \begin{pmatrix}
            2 & 1 & 1 \\
            1 & 3 & 2 \\
            1 & 0 & 0
        \end{pmatrix}.
    \end{equation*}
    Queremos escrever $A$ como um produto de matrizes elementares.
    
    Para isso, aplicamos operações elementares nas linhas de $A$ até que ela se transforme na matriz identidade:
    \begin{align*}
        A
        &\xrightarrow{L_1\leftrightarrow L_3}
        \begin{pmatrix}
            1 & 0 & 0 \\
            1 & 3 & 2 \\
            2 & 1 & 1
        \end{pmatrix}
        \xrightarrow{L_2\leftarrow L_2 - L_1}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 3 & 2 \\
            2 & 1 & 1
        \end{pmatrix}
        \xrightarrow{L_3\leftarrow L_3 - 2L_1}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 3 & 2 \\
            0 & 1 & 1
        \end{pmatrix} \\
        &\xrightarrow{L_2\leftarrow \frac{1}{3}L_2}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & \frac{2}{3} \\
            0 & 1 & 1
        \end{pmatrix}
        \xrightarrow{L_3\leftarrow L_3 - L_2}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & \frac{2}{3} \\
            0 & 0 & \frac{1}{3}
        \end{pmatrix}
        \xrightarrow{L_3\leftarrow 3L_3}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & \frac{2}{3} \\
            0 & 0 & 1
        \end{pmatrix} \\
        &\xrightarrow{L_2\leftarrow L_2 - \frac{2}{3}L_3}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{pmatrix}
        = I_3.
    \end{align*}
    Portanto, $A$ pode ser escrita como um produto de matrizes elementares.
    
    Temos que:
    \begin{equation*}
        A = E_1^{-1} E_2^{-1} E_3^{-1} E_4^{-1} E_5^{-1} E_6^{-1} E_7^{-1}.
    \end{equation*}

    Na expressão acima, $E_i^{-1}$ é a inversa da matriz elementar correspondente à $i$-ésima operação $e_i$ elementar aplicada.

    Se $e_i$ for uma troca de linhas, $E_i^{-1}=E_i$ é a matriz identidade com estas mesmas linhas trocadas.

    Se $e_i$ for uma multiplicação de uma linha por um escalar não nulo, $E_i^{-1}$ é a matriz identidade com a mesma linha multiplicada pelo inverso do escalar.

    Se $e_i$ for a adição de um múltiplo de uma linha a outra, $E_i^{-1}$ é a matriz identidade com o mesmo múltiplo da mesma linha subtraída da outra.
    Portanto,
    \begin{equation*}
        A = \begin{pmatrix}
            0 & 1 & 0 \\
            0 & 0 & 1 \\
            1 & 0 & 0
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            1 & 1 & 0 \\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            2 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 3 & 0 \\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 1 & 1
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & \frac{1}{3}
        \end{pmatrix}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & \frac{2}{3} \\
            0 & 0 & 1
        \end{pmatrix}.
    \end{equation*}
\end{example}

A demonstração do teorema anterior também nos dá um algoritmo para inverter matrizes: basta aplicar operações elementares nas linhas de $A$ até que ela se transforme em $I_n$.
A inversa será, então, a matriz $E_k\dots E_1$, onde $E_i$ é a matriz elementar correspondente à $i$-ésima operação elementar aplicada, $e_i$.
Porém, $E_k\dots E_1$ é a matriz $e_k\circ \dots \circ e_1(I_n)$, ou seja, é a matriz que resulta de aplicar as mesmas operações elementares em $I_n$.

Já se ao aplicar operações elementares em $A$ chegarmos em uma matriz escalonada diferente da identidade, esta não terá pivôs em todas as colunas (ou seria a identidade), e, portanto, o sistema linear homogêneo $AX=0$ possuirá soluções não triviais, o que, pela proposição, implica em $A$ não ser invertível.

\begin{example}
    Consideremos a seguinte matriz:
    \begin{equation*}
        A = \begin{pmatrix}
            1 & 0 & -1 \\
            2 & 1 & 1 \\
            1 & -1 & 0 \\
        \end{pmatrix}.
    \end{equation*}
    Para invertê-la, aplicaremos, simultaneamente, as mesmas operações elementares nela e em $I_4$, até que $A$ se transforme em $I_4$. A matriz a qual $I_4$ se transformar será $A^{-1}$.

    \begin{align*}
        &\left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            2 & 1 & 1  & 0 & 1 & 0 \\
            1 & -1 & 0 & 0 & 0 & 1 \\
        \end{array}
        \right)
        &
        \xrightarrow{L_2 \leftarrow L_2 - 2L_1}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            0 & 1 & 3  & -2 & 1 & 0 \\
            1 & -1 & 0 & 0 & 0 & 1 \\
        \end{array}
        \right) \\
        \xrightarrow{L_3 \leftarrow L_3 - L_1}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            0 & 1 & 3  & -2 & 1 & 0 \\
            0 & -1 & 1 & -1 & 0 & 1 \\
        \end{array}
        \right)
        &
        \xrightarrow{L_3 \leftarrow L_3 + L_2}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            0 & 1 & 3  & -2 & 1 & 0 \\
            0 & 0 & 4 & -3 & 1 & 1 \\
        \end{array}
        \right)
        \\
        \xrightarrow{L_3 \leftarrow \frac{1}{4}L_3}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            0 & 1 & 3  & -2 & 1 & 0 \\
            0 & 0 & 1 & -\frac{3}{4} & \frac{1}{4} & \frac{1}{4} \\
        \end{array}
        \right)
        &
        \xrightarrow{L_2 \leftarrow L_2 - 3L_3}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & -1 & 1 & 0 & 0 \\
            0 & 1 & 0  & \frac{1}{4} & \frac{1}{4} & -\frac{3}{4} \\
            0 & 0 & 1 & -\frac{3}{4} & \frac{1}{4} & \frac{1}{4} \\
        \end{array}
        \right)
        \\
        \xrightarrow{L_1 \leftarrow L_1 + L_3}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & 0 & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
            0 & 1 & 0  & \frac{1}{4} & \frac{1}{4} & -\frac{3}{4} \\
            0 & 0 & 1 & -\frac{3}{4} & \frac{1}{4} & \frac{1}{4} \\
        \end{array}
        \right)
    \end{align*}

    Portanto,
    \[
    A^{-1} =
    \begin{pmatrix}
        \frac{1}{4} & \frac{1}{4} & \frac{1}{4} \\
        \frac{1}{4} & \frac{1}{4} & -\frac{3}{4} \\
        -\frac{3}{4} & \frac{1}{4} & \frac{1}{4}
    \end{pmatrix}.
    \]
\end{example}

\begin{example}
    Consideremos a seguinte matriz:
    \begin{equation*}
        A = \begin{pmatrix}
            1 & 0 & 1 \\
            2 & 1 & 3 \\
            1 & -1 & 0 \\
        \end{pmatrix}.
    \end{equation*}
    Vamos tentar invertê-la.

    \begin{align*}
        &\left(
        \begin{array}{ccc|ccc}
            1 & 0 & 1 & 1 & 0 & 0 \\
            2 & 1 & 3 & 0 & 1 & 0 \\
            1 & -1 & 0 & 0 & 0 & 1 \\
        \end{array}
        \right)
        &
        \xrightarrow{L_2 \leftarrow L_2 - 2L_1}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & 1 & 1 & 0 & 0 \\
            0 & 1 & 1 & -2 & 1 & 0 \\
            1 & -1 & 0 & 0 & 0 & 1 \\
        \end{array}
        \right) \\
        \xrightarrow{L_3 \leftarrow L_3 - L_1}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & 1 & 1 & 0 & 0 \\
            0 & 1 & 1 & -2 & 1 & 0 \\
            0 & -1 & -1 & -1 & 0 & 1 \\
        \end{array}
        \right)
        &
        \xrightarrow{L_3 \leftarrow L_3 + L_2}
        &
        \left(
        \begin{array}{ccc|ccc}
            1 & 0 & 1 & 1 & 0 & 0 \\
            0 & 1 & 1 & -2 & 1 & 0 \\
            0 & 0 & 0 & -3 & 1 & 1 \\
        \end{array}
        \right)
    \end{align*}

    Observe que a última linha da matriz à esquerda ficou toda nula, enquanto a parte aumentada à direita não é nula.
    Isso indica que a forma escalonada de $A$ não é a identidade, e, portanto, $A$ não é invertível.
\end{example}

Agora vamos tirar mais alguns corolários teóricos a partir do já exposto.

\begin{corollary}
    Seja $A\in M_{n}(\mathbb K)$.
    As seguintes afirmações são equivalentes:
    \begin{enumerate}[label=(\roman*)]
        \item $A$ é invertível.
        \item $A$ é invertível à esquerda.
        \item $A$ é invertível à direita.
    \end{enumerate}
\end{corollary}
\begin{proof}
    A equivalência (i)$\Leftrightarrow$(ii) já foi estabelecida, e é claro que (i)$\Rightarrow$(iii).

    Para ver que (iii)$\Rightarrow$(i), suponha que $A$ é invertível à direita e seja $B\in M_{n}(\mathbb K)$ tal que $AB=I_n$.

    Segue que $B$ é invertível à esquerda, e, assim, $B$ é invertível e sua inversa é $A$.
    Logo, $A=B^{-1}$ é invertível (e sua inversa é $B$).
\end{proof}


\begin{corollary}
    Sejam $A_1, A_2, \dots, A_k\in M_{n}(\mathbb K)$ matrizes invertíveis.

    Se o produto $A_1 A_2 \cdots A_k$ é invertível, então cada matriz $A_i$ é invertível ($i=1, 2, \dots, k$).
\end{corollary}
\begin{proof}
Provaremos o corolário por indução em $k$.

Para $k=1$, o resultado é trivial.
Para $k=2$, suponha que $A_1 A_2$ é invertível.
Então, existe $B\in M_{n}(\mathbb K)$ tal que $B(A_1 A_2) = I_n$.
Assim, $B A_1$ é um inverso à esquerda de $A_2$, e, portanto, $A_2$ é invertível.
Além disso, $B$ é invertível, e, assim, $A_1=B^{-1}A_2^{-1}$ é invertível.

Suponha que o resultado seja válido para $k$.
Seja $A_1, A_2, \dots, A_{k+1}\in M_{n}(\mathbb K)$ tais que $A_1 A_2 \cdots A_{k+1}$ é invertível.
Pelo caso $k=2$, $A_{k+1}$ é invertível e $A_1 A_2 \cdots A_k$ é invertível, e então, por hipótese de indução, $A_1, A_2, \dots, A_k$ são invertíveis.
\end{proof}

